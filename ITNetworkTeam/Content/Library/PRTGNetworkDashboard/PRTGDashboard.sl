namespace: PRTGNetworkDashboard
operation:
  name: PRTGDashboard
  inputs:
    - username
    - passhash
    - svcbotUsername
    - svcbotPassword
  python_action:
    use_jython: false
    script: "def execute(username,passhash,svcbotUsername,svcbotPassword):\n    ## NOTE: Need to install pandas, msal, openpyxl\n    \n    from datetime import date, datetime, timedelta\n    from typing import Union\n    from math import isnan\n    import pandas as pd\n    \n    from msal import PublicClientApplication\n    \n    def authorisation(svcbotUsername, svcbotPassword):\n        #TO GET ACCESS TOKEN\n        app = PublicClientApplication(\"a7e41e17-18cf-4fd3-9dbb-d1322ad23e3f\", authority = \"https://login.microsoftonline.com/8c9b06d0-cb4a-4a03-b449-1f5a2548a910\")\n        result = None\n        accounts = app.get_accounts(username = \"utilities\\svcrpabot@sapowernetworks.com.au\")\n        if accounts:\n            result = app.acquire_token_silent(\"User.Read\", account=accounts[0])\n        if not result:\n            result = app.acquire_token_by_username_password(svcbotUsername, svcbotPassword, scopes=[\"User.Read\", \"Files.ReadWrite\"])\n        if \"access_token\" in result:\n            accessToken = result[\"access_token\"]\n        else:\n            accessToken = result.get(\"error\")\n        return accessToken\n    \n    \n    ## NOTE: Need to install pandas, msal, openpyxl\n    \n    import requests\n    import json\n    import pandas as pd\n    from io import BytesIO\n    \n    def search(accessToken: str, folderName: str, teamSite: str) -> str:\n        url = \"https://graph.microsoft.com/v1.0/sites/sapowernetworks.sharepoint.com:/sites/\" + teamSite + \"?$select=id\"\n        head = {\"Authorization\": \"Bearer \" + accessToken, \"Content-Type\": \"application/binary\"}\n        response = requests.get(url, headers=head)\n    \n        # Returns the HTTP Response and it's relative site ID\n        httpResponse = json.loads(response.text)\n        siteID = httpResponse[\"id\"]\n    \n    \n        # TRAVERSE SHAREPOINT - Document Library Folder\n        url = \"https://graph.microsoft.com/v1.0/sites/\" + siteID + \"/lists\"\n        head = {\"Authorization\": \"Bearer \" + accessToken, \"Content-Type\": \"application/binary\"}\n        response = requests.get(url, headers=head)\n    \n        # Returns the HTTP Response for the Document Library Folder\n        httpResponse = json.loads(response.text)\n    \n    \n        # Finds the ID of Document Library\n        documentLibrarySiteID = \"\"\n        for folder in httpResponse[\"value\"]:\n            if folder[\"name\"] == \"Shared Documents\":\n                documentLibrarySiteID = folder[\"id\"]\n                \n    \n        # TRAVERSE SHAREPOINT - General Folder\n        url = \"https://graph.microsoft.com/v1.0/sites/\" + siteID + \"/lists/\" + documentLibrarySiteID + \"/items\"\n        head = {\"Authorization\": \"Bearer \" + accessToken, \"Content-Type\": \"application/binary\"}\n        response = requests.get(url, headers=head)\n    \n        # Returns the HTTP Response for the General Folder\n        httpResponse = json.loads(response.text)\n    \n        # Finds the ID of General Folder\n        generalFolderID = \"\"\n        for folder in httpResponse[\"value\"]:\n            if folder[\"webUrl\"] == \"https://sapowernetworks.sharepoint.com/sites/\" + teamSite + \"/Shared%20Documents/General\":\n                generalFolderID = folder[\"id\"]\n    \n    \n        # Find all child folders in the General Folder\n        url = \"https://graph.microsoft.com/v1.0/sites/\" + siteID + \"/lists/\" + documentLibrarySiteID + \"/items/\" + generalFolderID + \"/driveItem/children\"\n        head = {\"Authorization\": \"Bearer \" + accessToken, \"Content-Type\": \"application/binary\"}\n        response = requests.get(url, headers=head)\n    \n        # Returns the HTTP Response for the child folder of the General Folder\n        httpResponse = json.loads(response.text)\n    \n        # Find the path of desired folder (i.e.PRTG-REPORTING-DO-NOT-DELETE)\n        folderPath = \"\"\n        for folder in httpResponse[\"value\"]:\n            if folder[\"name\"] == folderName:\n                folderPath = folder[\"parentReference\"][\"path\"]\n    \n        return folderPath\n    \n    \n    def download(accessToken: str, folderPath: str, folderName: str, fileName: str):\n        # Find all child files/folders within desired folder (i.e.PRTG-REPORTING-DO-NOT-DELETE)\n        url = \"https://graph.microsoft.com/v1.0\" + folderPath + \"/\" + folderName + \":/children\"\n        head = {\"Authorization\": \"Bearer \" + accessToken, \"Content-Type\": \"application/binary\"}\n        response = requests.get(url, headers=head)\n    \n        # Returns the HTTP Response for the desired files/folders\n        httpResponse = json.loads(response.text)\n    \n        # Loop through all returned files/folders and download the desired file. \n        downloadedFile = \"\"\n        for file in httpResponse[\"value\"]:\n            if file[\"name\"] == fileName:\n                if \".xlsx\" in fileName:\n                    downloadedFile = pd.read_excel(file[\"@microsoft.graph.downloadUrl\"])\n                    return downloadedFile\n                elif \".csv\" in fileName:\n                    downloadedFile = pd.read_csv(file[\"@microsoft.graph.downloadUrl\"])\n                    return downloadedFile\n        return pd.DataFrame()\n    \n    \n    \n    def upload(accessToken: str, df: pd, folderPath: str, folderName: str, fileName: str):\n        # Changes dataframe to excel file in memory and converts to binary\n        with BytesIO() as buffer:\n            if \".xlsx\" in fileName:\n                writer = pd.ExcelWriter(buffer, engine = 'xlsxwriter') \n                df.to_excel(writer, index=False, sheet_name='Sheet1')\n                writer.close()\n            elif \".csv\" in fileName:\n                df.to_csv(buffer, index=False)\n            file_contents = buffer.getvalue()\n    \n            \n    \n        # Construct the URL for the upload request\n        upload_url = \"https://graph.microsoft.com/v1.0\" + folderPath + \"/\" + folderName + \"/\" + fileName + \":/content\"\n    \n        # Set headers and body for upload request\n        headers = {\n            \"Authorization\": \"Bearer \" + accessToken,\n            \"Content-Type\": \"application/octet-stream\"\n        }\n    \n        # Make the upload request\n        response = requests.put(upload_url, headers=headers, data=file_contents)\n    \n        return response\n    \n    \n    # Make sure to have openpyxl installed\n    import pandas as pd\n    import csv\n    from typing import Union\n    \n    # Function to update output file\n    def returnOutput(siteIDMap, switchNameMap, switchIDMap, switchPingIDMap, routerNameMap, routerIDMap, routerPingIDMap, routerPICIDMap, downloadedFile):                             \n    \n        for (idx, row) in downloadedFile.iterrows():\n            # Update location ID in downloadedFile\n            downloadedFile.at[idx, \"Location ID\"] = str(siteIDMap[row[\"Location\"]])\n            \n            # Put router/switch names into an array\n            mainSwitchArray = switchNameMap[row[\"Location\"]]\n            try:\n                mainRouterArray = routerNameMap[row[\"Location\"]]\n            except:\n                print(row[\"Location\"] + \" does not have a router\")\n    \n             # Use to reference the index of the main switch\n            indexTracker = 0\n            for element in mainSwitchArray:\n                # Check if the element (reference) is the main switch\n                if (element == row[\"Main Switch\"]):\n                    # Add main switch ID\n                    mainSwitchID = switchIDMap[row[\"Location\"]][indexTracker]\n                    downloadedFile.at[idx, \"Switch ID\"] = str(mainSwitchID)\n    \n                    # Add main switch ping ID\n                    mainSwitchPingID = switchPingIDMap[row[\"Location\"]][indexTracker]\n                    downloadedFile.at[idx, \"Switch Ping ID\"] = str(mainSwitchPingID)\n                else:\n                    # Go to next element\n                    indexTracker = indexTracker + 1\n            \n             # Use to reference the index of the main router\n            indexTracker = 0\n            for element in mainRouterArray:\n                # Check if the element (reference) is the main router\n                if (element == row[\"Main Router\"]):\n                    # Add main router ID\n                    mainRouterID = routerIDMap[row[\"Location\"]][indexTracker]\n                    downloadedFile.at[idx, \"Router ID\"] = str(mainRouterID)\n    \n                    # Add main router ping ID\n                    mainRouterPingID = routerPingIDMap[row[\"Location\"]][indexTracker]\n                    downloadedFile.at[idx, \"Router Ping ID\"] = str(mainRouterPingID)\n    \n                    # Add main router PIC ID\n                    mainRouterPICID = routerPICIDMap[row[\"Location\"]][indexTracker]\n                    downloadedFile.at[idx, \"Router PIC ID\"] = str(mainRouterPICID)\n                else:\n                    # Go to next element\n                    indexTracker = indexTracker + 1\n    \n        return downloadedFile\n    \n    def getIDs(username: str, passHash: str, downloadedFile: pd) -> Union[dict, dict, dict, dict, dict, dict, dict, dict]:\n    \n        # Create the url for the API call.\n        url1 = \"https://prtg/api/table.csv?id=2012&count=*&columns=name,objid&username=\" + username + \"&passhash=\" + passHash\n        url2 = \"https://prtg/api/table.csv?id=13751&count=*&columns=name,objid&username=\" + username + \"&passhash=\" + passHash\n        url3 = \"https://prtg/api/table.csv?id=13694&count=*&columns=name,objid&username=\" + username + \"&passhash=\" + passHash\n    \n        # Attempt to download the API information from the PRTG network monitoring system into a pandas dataframe table\n        try:\n            apiData1 = pd.read_csv(url1)\n            apiData2 = pd.read_csv(url2)\n            apiData3 = pd.read_csv(url3) # Specific for the \"Internet\" site\n    \n        except:\n            # Return false if an API call was unsuccessful.\n            print(\"A critical error has occured.\")\n    \n        # Append both API calls into 1 dataframe\n        apiData = pd.concat([apiData1, apiData2])\n        apiData = pd.concat([apiData, apiData3])\n    \n        # Define and build the sites we need the ID of\n        sites = []\n        siteToRouterPICMapping = {}\n        sitesList = downloadedFile\n    \n        for index, row in sitesList[0:len(sitesList)].iterrows():\n            # each row is returned as a pandas series\n            sites.append(row['Location'])\n    \n            # Building router to PIC mapping to dynamically find sensor IDs (used below)\n            tempInfo = []\n            tempInfo.append(row['Main Router'])\n            tempInfo.append(row['Primary Internet Connection'])\n            siteToRouterPICMapping[row['Location']] = tempInfo\n    \n        # Define the structure to store site and ID\n        siteIDMap = {}\n    \n        # Build site ID mappings in dictionary\n        for site in sites:\n            # Find index of site in dataframes\n            indexValue1 = str(apiData1.index[apiData1['Object'] == site].tolist())\n            indexValue1 = indexValue1[1:]\n            indexValue1 = indexValue1[:-1]\n    \n            indexValue2 = str(apiData2.index[apiData2['Object'] == site].tolist())\n            indexValue2 = indexValue2[1:]\n            indexValue2 = indexValue2[:-1]\n    \n            indexValue3 = str(apiData3.index[apiData3['Object'] == site].tolist())\n            indexValue3 = indexValue3[1:]\n            indexValue3 = indexValue3[:-1]\n            \n            # Find ID per site\n            if(len(indexValue1) > 0):\n                dictValue = apiData1['ID'][int(indexValue1)]\n            elif(len(indexValue2) > 0):\n                dictValue = apiData2['ID'][int(indexValue2)]\n            else:\n                dictValue = apiData3['ID'][int(indexValue3)]\n            \n            # Create entries in dictionary\n            siteIDMap[site] = dictValue\n    \n        # Define the structure to store switch and router\n        routerIDMap = {}\n        switchIDMap = {}\n        routerNameMap = {}\n        switchNameMap = {}\n    \n        # Build ID and name mappings for switches and routers in dictionary\n        for site in sites:\n            # Call API depending on site ID\n            url4 = \"https://prtg/api/table.csv?id=\" + str(siteIDMap[site]) + \"&content=devices&columns=objid,name&count=*&username=\" + username + \"&passhash=\" + passHash\n            deviceData = pd.read_csv(url4)\n    \n            # Find index of all routers and switches in each API call\n            indexValue1 = deviceData.index[deviceData.Object.str.contains(\"-RT\")].tolist()\n            indexValue2 = deviceData.index[deviceData.Object.str.contains(\"-SW\")].tolist()\n    \n            # Declare temp variables\n            tempIDs1 = []\n            tempObjects1 = []\n            tempIDs2 = []\n            tempObjects2 = []\n    \n            # Build name and ID dictionary for each router per site\n            if(len(indexValue1) > 0):\n                # Iterate through the indices of all identified routers \n                for index in indexValue1:\n                    tempIDs1.append(deviceData['ID'][index])\n                    tempObjects1.append(deviceData['Object'][index])\n    \n                routerIDMap[site] = tempIDs1\n                routerNameMap[site] = tempObjects1\n    \n            # Build name and ID dictionary for each switch per site\n            if(len(indexValue2) > 0):\n                # Iterate through the indices of all identified switches\n                for index in indexValue2:\n                    tempIDs2.append(deviceData['ID'][index])\n                    tempObjects2.append(deviceData['Object'][index])\n    \n                switchIDMap[site] = tempIDs2\n                switchNameMap[site] = tempObjects2\n    \n        # Define the structure to store switch and router sensor IDs (PIC = Primary Internet Connection)\n        routerPingIDMap = {}\n        switchPingIDMap = {}\n        routerPICIDMap = {}\n    \n        # Build sensor ID mappings for switches and routers in dictionary\n        for site in sites:\n            try:\n                # Reference IDs from previous structures\n                switchID = switchIDMap[site]\n                routerID = routerIDMap[site]\n    \n                # Temp variables \n                tempPingIDs1 = []\n                tempPingIDs2 = []\n    \n                # Temp variables (PIC = Primary Internet Connection)\n                tempPICIDs1 = []\n    \n                # Find sensor IDs for each switch\n                for ID in switchID:\n                    # Call API to find information on specific switch\n                    url5 = \"https://prtg/api/table.csv?id=\" + str(ID) + \"&content=sensors&columns=objid,name&count=*&username=\" + username + \"&passhash=\" + passHash\n                    sensorData = pd.read_csv(url5)\n    \n                    # Find index of the Ping sensor\n                    indexValue1 = sensorData.index[sensorData.Object.str.contains(\"Ping\")].tolist()\n                    tempPingIDs1.append(sensorData['ID(RAW)'][indexValue1[0]])\n    \n                switchPingIDMap[site] = tempPingIDs1\n                \n                # Find sensor IDs for each router\n                for ID in routerID:\n                    # Call API to find information on specific switch\n                    url5 = \"https://prtg/api/table.csv?id=\" + str(ID) + \"&content=sensors&columns=objid,name&count=*&username=\" + username + \"&passhash=\" + passHash\n                    sensorData = pd.read_csv(url5)\n    \n                    # Find index of the Ping sensor\n                    indexValue1 = sensorData.index[sensorData.Object.str.contains(\"Ping\")].tolist()\n                    tempPingIDs2.append(sensorData['ID(RAW)'][indexValue1[0]]) \n    \n                    # Find index of primary internet connection sensor\n                    indexValue1 = sensorData.index[sensorData.Object.str.contains(siteToRouterPICMapping[site][1])].tolist()\n                    if(len(indexValue1) != 0):  \n                        tempPICIDs1.append(sensorData['ID(RAW)'][indexValue1[0]])\n                    else: \n                        tempPICIDs1.append(\"N/A\")\n                \n                # Assign key-value pairs in dictionary\n                routerPingIDMap[site] = tempPingIDs2\n                routerPICIDMap[site] = tempPICIDs1\n    \n            except:\n                print(site + \" does not have routers/switches\") \n    \n        # Hardcode for the \"Internet\" site, store everything in switch variables\n        url6 = \"https://prtg/api/table.csv?id=\" + str(siteIDMap['Internet']) + \"&content=devices&columns=objid,name&count=*&username=\" + username + \"&passhash=\" + passHash\n        deviceData = pd.read_csv(url6)\n        indexValue1 = deviceData.index[deviceData.Object.str.contains(\"www.google.com.au\")].tolist()\n        for index in indexValue1:\n            tempArray1 = []\n            tempArray1.append(deviceData['ID'][index])\n            switchIDMap['Internet'] = tempArray1\n            tempArray2 = []\n            tempArray2.append(deviceData['Object'][index])\n            switchNameMap['Internet'] = tempArray2\n    \n        url7 = \"https://prtg/api/table.csv?id=\" + str(switchIDMap['Internet'][0]) + \"&content=sensors&columns=objid,name&count=*&username=\" + username + \"&passhash=\" + passHash\n        sensorData = pd.read_csv(url7)\n        indexValue1 = sensorData.index[sensorData.Object.str.contains(\"Ping\")].tolist()\n        tempPingIDs1 = []\n        tempPingIDs1.append(sensorData['ID(RAW)'][indexValue1[0]])\n        switchPingIDMap[site] = tempPingIDs1\n        # End of hardcoded \"Internet\" site code section\n    \n        # Return data to main.py\n        return siteIDMap, switchNameMap, switchIDMap, switchPingIDMap, routerNameMap, routerIDMap, routerPingIDMap, routerPICIDMap\n    \n    \n    def downloadPRTGData(url: str) -> Union[bool, dict]:\n        # Attempts to connect to PRTG and download the data\n        try:\n            data = pd.read_csv(url)\n            return True, data\n        except:\n            # Return false if an API call was unsuccessful\n            print(\"A critical error has occured.\")\n            return False, None\n    \n    \n    def retrieveSharePointFolderPath(svcbotUsername, svcbotPassword) -> Union[str, str, str]:\n        # Retrieve the SharePoint session access token\n        accessToken = authorisation(svcbotUsername, svcbotPassword)\n    \n        # Retrieve the SharePoint Folder Path\n        FOLDERNAME = \"PRTG-REPORTING-DO-NOT-DELETE\"\n        TEAMSITE = \"CXNetworkTeam\"\n        folderPath = search(accessToken, FOLDERNAME, TEAMSITE)\n    \n        return accessToken, FOLDERNAME, folderPath\n    \n    \n    def downloadFileSharepoint(fileName: str, svcbotUsername, svcbotPassword) -> pd:\n        # Retrieve the SharePoint Access Token, Folder Name, & Folder Path\n        accessToken, FOLDERNAME, folderPath = retrieveSharePointFolderPath(svcbotUsername, svcbotPassword)\n    \n        # Download the Excel/CSV File\n        downloadedFile = download(accessToken, folderPath, FOLDERNAME, fileName)\n        \n        return downloadedFile\n    \n    \n    def uploadFileSharepoint(df: pd, fileName: str, svcbotUsername, svcbotPassword):\n        # Retrieve the SharePoint Access Token, Folder Name, & Folder Path\n        accessToken, FOLDERNAME, folderPath = retrieveSharePointFolderPath(svcbotUsername, svcbotPassword)\n    \n        # Download the Excel/CSV File\n        response = upload(accessToken, df, folderPath, FOLDERNAME, fileName)\n    \n        return response\n    \n    \n    def updatePRTGDeviceRegistry(svcbotUsername, svcbotPassword):\n        fileName = \"PRTG Device Registry.xlsx\"\n        downloadedFile = downloadFileSharepoint(fileName, svcbotUsername, svcbotPassword)\n    \n        siteIDMap, switchNameMap, switchIDMap, switchPingIDMap, routerNameMap, routerIDMap, routerPingIDMap, routerPICIDMap = getIDs(username, passhash, downloadedFile)\n    \n        result = returnOutput(siteIDMap, switchNameMap, switchIDMap, switchPingIDMap, routerNameMap, routerIDMap, routerPingIDMap, routerPICIDMap, downloadedFile)\n    \n        downloadedFile = uploadFileSharepoint(result, fileName, svcbotUsername, svcbotPassword)\n    \n    \n    \n    def networkUptimeReport(startDate: str, endDate: str, username: str, passhash: str, svcbotUsername, svcbotPassword):\n        newPRTGData = pd.DataFrame()\n        \n        # Download Network Routers from PRTG Device Registry\n        prtgDeviceRegistry = downloadFileSharepoint(\"PRTG Device Registry.xlsx\", svcbotUsername, svcbotPassword)\n    \n        # Loop through every device in \n        for index, row in prtgDeviceRegistry.iterrows():\n            if isnan(row[\"Switch Ping ID\"]) == False:    \n                print(row[\"Location\"])\n                print(str(int(row[\"Switch Ping ID\"])))\n                url = \"https://prtg/api/historicdata.csv?id=\" + str(int(row[\"Switch Ping ID\"])) + \"&avg=3600&sdate=\" + startDate + \"-00-00-00&edate=\" + endDate + \"-00-00-00&username=\" + username + \"&passhash=\" + passhash\n                apiCallSuccess, prtgData = downloadPRTGData(url)\n                \n                if apiCallSuccess == True:\n    \n                    # Add ID, location, and switch name to the table.\n                    prtgData.insert(0, \"Switch ID\", str(int(row[\"Switch Ping ID\"])), True)\n                    prtgData.insert(1, \"Location\", row[\"Location\"], True)\n                    prtgData.insert(2, \"Switch Name\", row[\"Main Switch\"], True)\n    \n                    # Remove last row from table\n                    prtgData.drop(prtgData.tail(1).index, inplace=True)\n    \n                    # Append Network Device Uptime Data     \n                    if newPRTGData.empty:\n                        newPRTGData = prtgData\n                    else:\n                        newPRTGData = pd.concat([newPRTGData, prtgData], axis=0)\n    \n        # Download existing Network Uptime Data from SharePoint\n        existingPRTGData = downloadFileSharepoint(\"Network Uptime Data.csv\", svcbotUsername, svcbotPassword)\n    \n        # If previous PRTG Data exists on SharePoint then merge the old and new data else upload only the new PRTG data\n        if existingPRTGData.empty == False:\n            # Merge the existing PRTG data with the new PRTG data\n            concatenated_df = pd.concat([existingPRTGData, newPRTGData]).drop_duplicates(['Location', 'Date Time'])\n    \n            # Order by date\n            concatenated_df.sort_values('Date Time', ascending=False)\n    \n            # Upload file to SharePoint\n            print(concatenated_df)\n            uploadFileSharepoint(concatenated_df, \"Network Uptime Data.csv\", svcbotUsername, svcbotPassword)\n        else:\n            # Upload the new PRTG data\n            print(newPRTGData)\n            uploadFileSharepoint(newPRTGData, \"Network Uptime Data.csv\", svcbotUsername, svcbotPassword)\n    \n    \n    def networkUtilisationReport(startDate: str, endDate: str, username: str, passhash: str, svcbotUsername, svcbotPassword):\n    \n        newPRTGData = pd.DataFrame()\n        \n        # Download Network Routers from PRTG Device Registry\n        prtgDeviceRegistry = downloadFileSharepoint(\"PRTG Device Registry.xlsx\", svcbotUsername, svcbotPassword)\n    \n        # Loop through every row within the PRTG Device Registry\n        for index, row in prtgDeviceRegistry.iterrows():\n            if isnan(row[\"Router PIC ID\"]) == False:\n            \n                print(row[\"Location\"])\n                print(str(int(row[\"Router PIC ID\"])))\n                url = \"https://prtg/api/historicdata.csv?id=\" + str(int(row[\"Router PIC ID\"])) + \"&avg=3600&sdate=\" + startDate + \"-00-00-00&edate=\" + endDate + \"-00-00-00&username=\" + username + \"&passhash=\" + passhash\n                apiCallSuccess, prtgData = downloadPRTGData(url)\n    \n                # Clean Data\n                if apiCallSuccess == True:\n                    \n                    # Add ID, location, router name, and site bandwidth to the table.\n                    prtgData.insert(0, \"Router ID\", str(int(row[\"Router ID\"])), True)\n                    prtgData.insert(1, \"Location\", row[\"Abbreviated Location\"], True)\n                    prtgData.insert(2, \"Router Name\", row[\"Main Router\"], True)\n                    prtgData.insert(3, \"Site Bandwidth\", row[\"Site Bandwidth\"], True)\n                    \n                    # Remove last 2 unneccessary rows from table\n                    prtgData.drop(prtgData.tail(2).index, inplace=True)\n    \n                    # Append Network Utilisation Data    \n                    if newPRTGData.empty:\n                        newPRTGData = prtgData\n                    else:\n                        newPRTGData = pd.concat([newPRTGData, prtgData], axis=0)\n    \n        # Select only the necessary columns for output.\n        newPRTGData.drop(columns=newPRTGData.columns.difference(['Router ID','Location', 'Router Name', 'Site Bandwidth', 'Date Time', 'Traffic Total (volume)', \\\n                'Traffic Total (volume)(RAW)', 'Traffic Total (speed)', 'Traffic Total (speed)(RAW)', 'Traffic In (volume)', 'Traffic In (volume)(RAW)', 'Traffic In (speed)', \\\n                'Traffic In (speed)(RAW)', 'Traffic Out (volume)', 'Traffic Out (volume)(RAW)', 'Traffic Out (speed)', 'Traffic Out (speed)(RAW)']), inplace=True)\n        \n        # Download existing Network Utilisation Data from SharePoint\n        existingPRTGData = downloadFileSharepoint(\"Network Utilisation Data.csv\", svcbotUsername, svcbotPassword)\n    \n        # If previous PRTG Data exists on SharePoint then merge the old and new data else upload only the new PRTG data\n        if existingPRTGData.empty == False:\n            # Merge the existing PRTG data with the new PRTG data\n            concatenated_df = pd.concat([existingPRTGData, newPRTGData]).drop_duplicates(['Location', 'Date Time'])\n    \n            # Order by date\n            concatenated_df.sort_values('Date Time', ascending=False)\n    \n            # Upload file to SharePoint\n            print(concatenated_df)\n            uploadFileSharepoint(concatenated_df, \"Network Utilisation Data.csv\", svcbotUsername, svcbotPassword)\n        else:\n            # Upload the new PRTG data\n            print(newPRTGData)\n            uploadFileSharepoint(newPRTGData, \"Network Utilisation Data.csv\", svcbotUsername, svcbotPassword)\n    \n    \n    def networkOverviewReport(username: str, passhash: str, svcbotUsername, svcbotPassword):\n        # PRTG Download URL\n        url = \"https://prtg/api/table.csv?content=sensors&output=csvtable&columns=objid,name,probe,group,favorite,lastvalue,device,downtime,downtimetime,downtimesince,uptime,uptimetime,uptimesince,knowntime&count=*&username=\" + username + \"&passhash=\" + passhash\n    \n        # Downloads the Network Overview csv file from PRTG\n        apiCallSuccess, data = downloadPRTGData(url)\n        \n        # Uploads the file to SharePoint if successful, overwriting the pre-existing file\n        if apiCallSuccess == True:\n            uploadFileSharepoint(data, \"Network Overview Data.csv\", svcbotUsername, svcbotPassword)\n    \n    \n    \n    ### MAIN ###\n    # Retrieve the start and end date of the weekly report (Format: YYYY-MM-DD).\n    startDate = str(date.today() - timedelta(days=2))\n    endDate = str(date.today())\n    \n    # Update the PRTG Device Registry \n    updatePRTGDeviceRegistry(svcbotUsername, svcbotPassword)\n    \n    # # Create PowerBI Reports\n    networkOverviewReport(username, passhash, svcbotUsername, svcbotPassword)\n    networkUptimeReport(startDate, endDate, username, passhash, svcbotUsername, svcbotPassword)\n    networkUtilisationReport(startDate, endDate, username, passhash, svcbotUsername, svcbotPassword)"
  results:
    - SUCCESS
